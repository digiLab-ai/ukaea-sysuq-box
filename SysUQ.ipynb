{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKGo5GCPf2vu"
      },
      "source": [
        "# Finite Element Surrogate Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Dytv0NH3vO"
      },
      "source": [
        "# 0. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdIpbXMUH7FD",
        "outputId": "9d2078ad-867b-4e7b-98f8-0eca6177de65"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Av3OFVIGgb"
      },
      "source": [
        "In the *root* directory of your project, make a file named `.env`. Populate the file with:\n",
        "\n",
        "```\n",
        "UE_USERNAME='your_username'\n",
        "UE_PASSWORD='your_password'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-MRbk2uI1b8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from io import BytesIO, StringIO\n",
        "import urllib.request\n",
        "import requests\n",
        "from pprint import pprint\n",
        "\n",
        "from bisect import bisect_right\n",
        "\n",
        "from typing import Tuple, Dict, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import uncertainty_engine as engine\n",
        "\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxpvQQwDH-Gv"
      },
      "outputs": [],
      "source": [
        "# automatically load .env in current directory\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caKzVKu0M2Oc"
      },
      "source": [
        "We can now tell the Uncertainty Engine who we are and which project we want to work in. For now, we will assume your personal project, but you can add a new project using the GUI: https://uncertaintyengine.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV0dQ3CLM0ng"
      },
      "outputs": [],
      "source": [
        "client = engine.Client()\n",
        "client.authenticate()\n",
        "PROJECT_NAME = \"Personal\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROfInmvhgFR4"
      },
      "source": [
        "# 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELPMsRErmn87"
      },
      "outputs": [],
      "source": [
        "models_directory = 'models'\n",
        "data_directory = 'data'\n",
        "raw_data_directory = f'{data_directory}{os.sep}IN_OUT'\n",
        "input_filename = 'collated_inputs.csv'\n",
        "output_filename = 'collated_outputs.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpG-QNchburr"
      },
      "outputs": [],
      "source": [
        "def read_auto_csv(filepath, is_overwrite=False):\n",
        "    with open(filepath, 'r', newline='', encoding='utf-8') as f:\n",
        "        sample = f.read(2048)  # small chunk to analyse\n",
        "\n",
        "    # Detect the delimiter\n",
        "    dialect = csv.Sniffer().sniff(sample, delimiters=[',',';','\\t'])\n",
        "    delimiter = dialect.delimiter\n",
        "    df = pd.read_csv(filepath, delimiter=delimiter)\n",
        "    if delimiter != ',' and is_overwrite:\n",
        "        print(f\"saving: {filepath}\")\n",
        "        df.to_csv(filepath, index=False)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm6WjDnXgWO1"
      },
      "outputs": [],
      "source": [
        "is_clean = True\n",
        "if is_clean:\n",
        "    # Folder path\n",
        "    folder = Path(raw_data_directory)\n",
        "    # Iterate through matching files\n",
        "    for file in folder.glob('*.csv'):\n",
        "        read_auto_csv(file, is_overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwHaLMb9ga_A"
      },
      "outputs": [],
      "source": [
        "def load_numeric_csv(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    print(f\"Processing: {filepath}\")\n",
        "    print(f\"\\t shape: {df.shape}\")\n",
        "    df.columns = df.columns.str.strip()\n",
        "\n",
        "    df_numeric = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    non_numeric_cols = df.columns[(df_numeric.isna() & df.notna()).any()]\n",
        "    if len(non_numeric_cols) > 0:\n",
        "        print(f\"Non-numeric column(s) found, dropping:\", list(non_numeric_cols))\n",
        "        df = df.drop(columns=list(non_numeric_cols))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def make_input_features_from_file(\n",
        "    filepath,\n",
        "    time_col: str = \"Time\",\n",
        "    input_cols=(\n",
        "        \"Convection_FilmCoef\",\n",
        "        \"Heat Flux_Magnitude\",\n",
        "        \"Convection_AmbientTemp\",\n",
        "    ),\n",
        "):\n",
        "    \"\"\"\n",
        "    Load input features from a CSV and return:\n",
        "        [Time, input_cols..., source_file]\n",
        "\n",
        "    No rolling time windows, just the original values.\n",
        "    \"\"\"\n",
        "    df = load_numeric_csv(filepath)\n",
        "\n",
        "    # Check required columns\n",
        "    required = list(input_cols) + [time_col]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in {filepath}: {missing}\")\n",
        "\n",
        "    # Keep only Time + input columns (in a controlled order)\n",
        "    feats = df[[time_col, *input_cols]].copy()\n",
        "\n",
        "    # Optional: drop first row to match previous convention\n",
        "    # (e.g. if outputs lose the first row due to diffs/lag)\n",
        "    feats = feats.iloc[1:].copy()\n",
        "\n",
        "    # Keep track of source file\n",
        "    feats[\"source_file\"] = Path(filepath).name\n",
        "\n",
        "    # Reset index to simple 0..N-1\n",
        "    return feats.reset_index(drop=True)\n",
        "\n",
        "\n",
        "def make_output_features_from_file(\n",
        "    filepath,\n",
        "    time_col=\"Time\",\n",
        "    temp_cols=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Return raw temperature fields (no derivatives).\n",
        "\n",
        "    Each row = one time step for this simulation.\n",
        "    Columns = all temperature vertices + Time + source_file.\n",
        "    \"\"\"\n",
        "    df = load_numeric_csv(filepath)\n",
        "\n",
        "    if time_col not in df.columns:\n",
        "        raise ValueError(f\"{time_col} column is required in {filepath}\")\n",
        "\n",
        "    # Auto-detect temperature columns if not provided\n",
        "    if temp_cols is None:\n",
        "        temp_cols = [c for c in df.columns if c != time_col]\n",
        "\n",
        "    # TimedeltaIndex (mainly for consistency; not strictly needed here)\n",
        "    t = pd.to_timedelta(df[time_col].values, unit=\"s\")\n",
        "    df_time = df.set_index(t)\n",
        "\n",
        "    out = df_time[temp_cols].copy()\n",
        "\n",
        "    # Add aligned time and source\n",
        "    out[time_col] = df_time[time_col].values\n",
        "    out[\"source_file\"] = Path(filepath).name\n",
        "\n",
        "    return out.reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFA8uS9lgfEl"
      },
      "outputs": [],
      "source": [
        "def extract_number(path: Path):\n",
        "    \"\"\"\n",
        "    Extract the digits between '_' and '_' for ordering.\n",
        "    Example: 'abc_123_xyz.csv' → 123\n",
        "\n",
        "    Returns float('inf') if no match is found.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"_(\\d+)_\", path.name)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return float(\"inf\")\n",
        "\n",
        "\n",
        "_SCENARIO_RE = re.compile(r\"Scenario_(\\d+)\", re.IGNORECASE)\n",
        "\n",
        "def _scenario_id_from_name(pathlike) -> int | None:\n",
        "    \"\"\"Extract X from filenames like Scenario_X_exc.csv / Scenario_X_out.csv.\"\"\"\n",
        "    m = _SCENARIO_RE.search(Path(pathlike).name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "\n",
        "def collate_paired_datasets(\n",
        "    foldername,\n",
        "    input_pattern=\"*_exc.csv\",\n",
        "    output_pattern=\"*out.csv\",\n",
        "    time_col=\"Time\",\n",
        "    input_cols=(\n",
        "        \"Convection_FilmCoef\",\n",
        "        \"Heat Flux_Magnitude\",\n",
        "        \"Convection_AmbientTemp\",\n",
        "    ),\n",
        "    temp_cols=None,\n",
        "    include_scenarios=None,\n",
        "):\n",
        "    folder = Path(foldername)\n",
        "\n",
        "    # Gather files\n",
        "    input_files_all = list(folder.glob(input_pattern))\n",
        "    output_files_all = list(folder.glob(output_pattern))\n",
        "\n",
        "    # Build {scenario_id: filepath} maps (one-to-one expectation)\n",
        "    in_map = {}\n",
        "    out_map = {}\n",
        "\n",
        "    for f in input_files_all:\n",
        "        sid = _scenario_id_from_name(f)\n",
        "        if sid is None:\n",
        "            continue\n",
        "        if sid in in_map:\n",
        "            raise ValueError(f\"Multiple input files found for Scenario_{sid}: {in_map[sid].name}, {f.name}\")\n",
        "        in_map[sid] = f\n",
        "\n",
        "    for f in output_files_all:\n",
        "        sid = _scenario_id_from_name(f)\n",
        "        if sid is None:\n",
        "            continue\n",
        "        if sid in out_map:\n",
        "            raise ValueError(f\"Multiple output files found for Scenario_{sid}: {out_map[sid].name}, {f.name}\")\n",
        "        out_map[sid] = f\n",
        "\n",
        "    # Decide which scenarios to include\n",
        "    if include_scenarios is None:\n",
        "        scenario_ids = sorted(set(in_map.keys()) & set(out_map.keys()))\n",
        "    else:\n",
        "        scenario_ids = sorted(set(int(s) for s in include_scenarios))\n",
        "\n",
        "    # Check coverage / mismatches\n",
        "    missing_in = [sid for sid in scenario_ids if sid not in in_map]\n",
        "    missing_out = [sid for sid in scenario_ids if sid not in out_map]\n",
        "    if missing_in or missing_out:\n",
        "        msg = []\n",
        "        if missing_in:\n",
        "            msg.append(f\"missing input files for scenarios: {missing_in}\")\n",
        "        if missing_out:\n",
        "            msg.append(f\"missing output files for scenarios: {missing_out}\")\n",
        "        raise ValueError(\"Cannot form pairs for requested scenarios; \" + \"; \".join(msg))\n",
        "\n",
        "    # Create ordered, paired lists\n",
        "    input_files = [in_map[sid] for sid in scenario_ids]\n",
        "    output_files = [out_map[sid] for sid in scenario_ids]\n",
        "\n",
        "    print(f\"Including scenarios: {scenario_ids}\")\n",
        "\n",
        "    all_inputs = []\n",
        "    all_outputs = []\n",
        "\n",
        "    for sid, in_file, out_file in zip(scenario_ids, input_files, output_files):\n",
        "        print(f\"\\n=== Pair (Scenario_{sid}) ===\\nInput:  {in_file.name}\\nOutput: {out_file.name}\")\n",
        "\n",
        "        X_i = make_input_features_from_file(\n",
        "            in_file,\n",
        "            time_col=time_col,\n",
        "            input_cols=input_cols,\n",
        "        )\n",
        "        y_i = make_output_features_from_file(\n",
        "            out_file,\n",
        "            time_col=time_col,\n",
        "            temp_cols=temp_cols,\n",
        "        )\n",
        "\n",
        "        # Check length alignment; if mismatch, truncate to the shorter of the two\n",
        "        if len(X_i) != len(y_i):\n",
        "            print(\n",
        "                f\"WARNING: length mismatch for Scenario_{sid} \"\n",
        "                f\"{in_file.name} / {out_file.name}: \"\n",
        "                f\"{len(X_i)} (inputs) vs {len(y_i)} (outputs)\"\n",
        "            )\n",
        "            n = min(len(X_i), len(y_i))\n",
        "            X_i = X_i.iloc[:n].copy()\n",
        "            y_i = y_i.iloc[:n].copy()\n",
        "\n",
        "        all_inputs.append(X_i)\n",
        "        all_outputs.append(y_i)\n",
        "\n",
        "    if not all_inputs or not all_outputs:\n",
        "        raise ValueError(\"No paired datasets were found to collate (check patterns and/or include_scenarios).\")\n",
        "\n",
        "    collated_inputs = pd.concat(all_inputs, ignore_index=True)\n",
        "    collated_outputs = pd.concat(all_outputs, ignore_index=True)\n",
        "\n",
        "    print(\"\\nFinal collated shapes:\")\n",
        "    print(f\"  Inputs:  {collated_inputs.shape}\")\n",
        "    print(f\"  Outputs: {collated_outputs.shape}\")\n",
        "\n",
        "    # Show which subdatasets contribute (unique source files)\n",
        "    if \"source_file\" in collated_inputs.columns:\n",
        "        print(\"\\nInput source files contributing:\")\n",
        "        print(collated_inputs[\"source_file\"].value_counts())\n",
        "    if \"source_file\" in collated_outputs.columns:\n",
        "        print(\"\\nOutput source files contributing:\")\n",
        "        print(collated_outputs[\"source_file\"].value_counts())\n",
        "\n",
        "    return collated_inputs, collated_outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnhhDfXBk2MF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_latent_training_set(\n",
        "    collated_inputs: pd.DataFrame,\n",
        "    collated_outputs: pd.DataFrame,\n",
        "    time_col: str = \"Time\",\n",
        "    n_latent: int = 6,\n",
        "    lag_s: float = 0.1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build (X, y) for the GP:\n",
        "\n",
        "      X = [external inputs at time t, latent block state at t - lag_s]\n",
        "      y = latent block state at time t\n",
        "\n",
        "    where the latent block state is a low-rank linear projection of the\n",
        "    4960-vertex temperature field down to n_latent dimensions.\n",
        "\n",
        "    IMPORTANT:\n",
        "    ----------\n",
        "    This uses TruncatedSVD.\n",
        "    It's effectively an SVD-based latent basis on the\n",
        "    raw temperature fields.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Identify temperature columns (4960 vertices)\n",
        "    temp_cols = [\n",
        "        c for c in collated_outputs.columns\n",
        "        if c not in (time_col, \"source_file\")\n",
        "    ]\n",
        "\n",
        "    # 2) SVD-like decomposition on the full temperature space (no centering)\n",
        "    T_full = collated_outputs[temp_cols].values  # shape: (N, 4960)\n",
        "\n",
        "    # TruncatedSVD behaves like PCA without centering\n",
        "    pca = TruncatedSVD(n_components=n_latent, random_state=0)\n",
        "    Z = pca.fit_transform(T_full)   # shape: (N, n_latent)\n",
        "\n",
        "    latent_cols = [f\"T_latent_{k}\" for k in range(n_latent)]\n",
        "    Z_df = pd.DataFrame(Z, columns=latent_cols)\n",
        "\n",
        "    # 3) Build base table with:\n",
        "    #    - external inputs (without Time/source_file)\n",
        "    #    - Time + source_file (once, from outputs)\n",
        "    #    - latent coordinates\n",
        "    input_meta_drop = [c for c in (time_col, \"source_file\") if c in collated_inputs.columns]\n",
        "    inputs_clean = collated_inputs.drop(columns=input_meta_drop, errors=\"ignore\").reset_index(drop=True)\n",
        "\n",
        "    meta = collated_outputs[[time_col, \"source_file\"]].reset_index(drop=True)\n",
        "\n",
        "    base = pd.concat(\n",
        "        [\n",
        "            inputs_clean,\n",
        "            meta,\n",
        "            Z_df,\n",
        "        ],\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    # 4) For each simulation (source_file), create lagged latent features\n",
        "    def add_lag_per_file(g: pd.DataFrame) -> pd.DataFrame:\n",
        "        g = g.sort_values(time_col).reset_index(drop=True)\n",
        "\n",
        "        # Estimate a representative dt for this run\n",
        "        dt = g[time_col].diff().median()  # seconds\n",
        "        if dt <= 0 or pd.isna(dt):\n",
        "            raise ValueError(\n",
        "                f\"Non-positive or NaN dt detected in group {g['source_file'].iloc[0]}\"\n",
        "            )\n",
        "\n",
        "        lag_steps = int(round(lag_s / dt))\n",
        "        if lag_steps < 1:\n",
        "            lag_steps = 1  # at least one step\n",
        "\n",
        "        # Create lagged latent cols\n",
        "        for col in latent_cols:\n",
        "            g[f\"{col}_lag\"] = g[col].shift(lag_steps)\n",
        "\n",
        "        return g\n",
        "\n",
        "    base = base.groupby(\"source_file\", group_keys=False).apply(add_lag_per_file)\n",
        "\n",
        "    # 5) Drop rows without a full lag history (start of each simulation)\n",
        "    lag_cols = [f\"{c}_lag\" for c in latent_cols]\n",
        "    base = base.dropna(subset=lag_cols).reset_index(drop=True)\n",
        "\n",
        "    # 6) Build X and y arrays\n",
        "    external_input_cols = [\n",
        "        c for c in inputs_clean.columns\n",
        "        if c not in (time_col, \"source_file\")\n",
        "    ]\n",
        "\n",
        "    X_cols = external_input_cols + lag_cols\n",
        "    y_cols = latent_cols\n",
        "\n",
        "    X = base[X_cols].to_numpy()\n",
        "    y = base[y_cols].to_numpy()\n",
        "\n",
        "    print(\"Final training arrays:\")\n",
        "    print(\"  X shape:\", X.shape, \"(external + latent_lag)\")\n",
        "    print(\"  y shape:\", y.shape, \"(current latent)\")\n",
        "\n",
        "    return X, y, pca, temp_cols, X_cols, y_cols\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APodvo9nEsBw"
      },
      "outputs": [],
      "source": [
        "train_indices = np.hstack(([3], np.arange(5, 18), np.arange(26, 40)))\n",
        "validate_indices = np.arange(18, 26)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A766cZt7k8AP",
        "outputId": "56e69f06-d19d-4e16-fa80-326665d956c0"
      },
      "outputs": [],
      "source": [
        "collated_train_inputs, collated_train_outputs = collate_paired_datasets(\n",
        "    foldername=raw_data_filepath,\n",
        "    input_pattern=\"*_exc.csv\",\n",
        "    output_pattern=\"*out.csv\",\n",
        "    time_col=\"Time\",\n",
        "    input_cols=(\n",
        "        \"Convection_FilmCoef\",\n",
        "        \"Heat Flux_Magnitude\",\n",
        "        \"Convection_AmbientTemp\",\n",
        "    ),\n",
        "    include_scenarios=train_indices,\n",
        ")\n",
        "\n",
        "collated_validate_inputs, collated_validate_outputs = collate_paired_datasets(\n",
        "    foldername=raw_data_filepath,\n",
        "    input_pattern=\"*_exc.csv\",\n",
        "    output_pattern=\"*out.csv\",\n",
        "    time_col=\"Time\",\n",
        "    input_cols=(\n",
        "        \"Convection_FilmCoef\",\n",
        "        \"Heat Flux_Magnitude\",\n",
        "        \"Convection_AmbientTemp\",\n",
        "    ),\n",
        "    include_scenarios=validate_indices,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1fWQY3Q92j2"
      },
      "outputs": [],
      "source": [
        "# split into"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLpIwjXjk_M5",
        "outputId": "4e5e90a0-c288-4e03-d212-11b5ee122cb9"
      },
      "outputs": [],
      "source": [
        "n_latent = 6\n",
        "X, y, pca, temp_cols, X_cols, y_cols = build_latent_training_set(\n",
        "    collated_train_inputs,\n",
        "    collated_train_outputs,\n",
        "    time_col=\"Time\",\n",
        "    n_latent=n_latent,\n",
        "    lag_s=1.,\n",
        ")\n",
        "pca_matrix = pca.components_[:n_latent]\n",
        "\n",
        "collated_inputs = pd.DataFrame(X, columns=X_cols)\n",
        "collated_outputs = pd.DataFrame(y, columns=y_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKDQ-FHlHQlJ"
      },
      "outputs": [],
      "source": [
        "n_thin = 1000\n",
        "idx = collated_inputs.sample(n=n_thin, random_state=42).index\n",
        "collated_inputs = collated_inputs.loc[idx]\n",
        "collated_outputs = collated_outputs.loc[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4xh8P6Z0RoF"
      },
      "outputs": [],
      "source": [
        "collated_inputs.to_csv(f\"inputs.csv\", index=False)\n",
        "collated_outputs.to_csv(f\"outputs.csv\", index=False)\n",
        "pd.DataFrame(pca_matrix).to_csv(f\"pca_matrix.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "6Qcr8gVrGVf2",
        "outputId": "95327f94-319e-4efd-8840-2a920b8192ae"
      },
      "outputs": [],
      "source": [
        "collated_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "GCkMKajwT-by",
        "outputId": "492aa97c-047e-4401-e369-8655223e8db3"
      },
      "outputs": [],
      "source": [
        "collated_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sPEF3eJsMtdX",
        "outputId": "633724fe-f3d2-4213-b4f9-f61558f07ab9"
      },
      "outputs": [],
      "source": [
        "# Upload the resources\n",
        "\n",
        "client.resources.upload(\n",
        "    project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "    name=input_filename,\n",
        "    resource_type=\"dataset\",\n",
        "    file_path=f\"{data_directory}{os.sep}{input_filename}.csv\",\n",
        ")\n",
        "client.resources.upload(\n",
        "    project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "    name=output_filename,\n",
        "    resource_type=\"dataset\",\n",
        "    file_path=f\"{data_directory}{os.sep}{output_filename}.csv\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgx95dpkHxbn"
      },
      "source": [
        "# 2. Training The Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APirEOo2GiBP"
      },
      "outputs": [],
      "source": [
        "def get_presigned_url(url):\n",
        "    \"\"\"\n",
        "    Get the contents from the presigned url.\n",
        "    \"\"\"\n",
        "    url = url.replace(\"https://\", \"http://\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "    return response\n",
        "\n",
        "def get_results_dataframe(response):\n",
        "    \"\"\"\n",
        "    Get the results dataframe from the workflow response.\n",
        "    \"\"\"\n",
        "    returns = {}\n",
        "    for key, value in response.outputs[\"outputs\"].items():\n",
        "        response = get_presigned_url(value)\n",
        "        df = pd.read_csv(StringIO(response.text))\n",
        "        returns[key] = df\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CBxMmDKlFR"
      },
      "outputs": [],
      "source": [
        "from uncertainty_engine.nodes.base import Node\n",
        "from uncertainty_engine.nodes.resource_management import LoadDataset\n",
        "from uncertainty_engine.graph import Graph\n",
        "from uncertainty_engine.nodes.workflow import Workflow\n",
        "\n",
        "def get_presigned_url(url):\n",
        "    \"\"\"\n",
        "    Get the contents from the presigned url.\n",
        "    \"\"\"\n",
        "    url = url.replace(\"https://\", \"http://\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "    return response\n",
        "\n",
        "def train_export_torchscript_workflow(\n",
        "                    client,\n",
        "                   project_name: str,\n",
        "                   input_dataset_name: str,\n",
        "                   output_dataset_name: str,\n",
        "                    output_column_name: str,\n",
        "                   save_model_name: str,\n",
        "                   is_visualise_workflow: bool = False,\n",
        "                   is_print_full_output: bool = False) -> dict:\n",
        "    \"\"\"\n",
        "    A workflow that trains a machine learning model.\n",
        "    Here, we assume all resources have already been uploaded to the cloud.\n",
        "    :param client: The Uncertainty Engine client.\n",
        "    :param project_name: The name of the project.\n",
        "    :param dataset_name: The name of the dataset.\n",
        "    :param input_names: The names of the input columns.\n",
        "    :param output_names: The names of the output columns.\n",
        "    :param save_model_name: The name to save the trained model as.\n",
        "    :param is_visualise_workflow: Whether to print the workflow graph.\n",
        "    :param is_print_full_output: Whether to print the full output of the workflow.\n",
        "    :return: The response from running the workflow.\n",
        "    \"\"\"\n",
        "    # 1. Create the graph\n",
        "    graph = Graph()\n",
        "\n",
        "    # 2. Create relevant nodes, handles, and add to graph:\n",
        "\n",
        "    # 2.a. Load input dataset node\n",
        "    load_input_data = LoadDataset(\n",
        "        label=\"Load Input Dataset\",\n",
        "        file_id=client.resources.get_resource_id_by_name(\n",
        "            name=input_dataset_name,\n",
        "            project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "            resource_type='dataset'\n",
        "        ),\n",
        "        project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(load_input_data)  # add to graph\n",
        "    input_dataset = load_input_data.make_handle(\"file\")  # add handle\n",
        "\n",
        "    # 2.b. Load output dataset node\n",
        "    load_output_data = LoadDataset(\n",
        "        label=\"Load Output Dataset\",\n",
        "        file_id=client.resources.get_resource_id_by_name(\n",
        "            name=output_dataset_name,\n",
        "            project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "            resource_type='dataset'\n",
        "        ),\n",
        "        project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(load_output_data)  # add to graph\n",
        "    output_dataset = load_output_data.make_handle(\"file\")  # add handle\n",
        "\n",
        "    # 2.c. Filter dataset node for outputs\n",
        "    output_dataset_column = Node(\n",
        "        node_name=\"FilterDataset\",\n",
        "        label=\"Output Dataset\",\n",
        "        columns=[output_column_name],\n",
        "        dataset=output_dataset,\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(output_dataset_column)  # add to graph\n",
        "    output_dataset_columns = output_dataset_column.make_handle(\"dataset\")  # add handle\n",
        "\n",
        "    # 2.d. Model config node\n",
        "    model_config = Node(\n",
        "        node_name=\"ModelConfig\",\n",
        "        label=\"Model Config\",\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(model_config)  # add to graph\n",
        "    output_config = model_config.make_handle(\"config\")  # add handle\n",
        "\n",
        "    # 2.e. Train model node\n",
        "    train_model = Node(\n",
        "        node_name=\"TrainModel\",\n",
        "        label=\"Train Model\",\n",
        "        config=output_config,\n",
        "        inputs=input_dataset,\n",
        "        outputs=output_dataset_columns,\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(train_model)  # add to graph\n",
        "    output_model = train_model.make_handle(\"model\")  # add handle\n",
        "\n",
        "    # 2.f. Save model node\n",
        "    save = Node(\n",
        "        node_name=\"Save\",\n",
        "        label=\"Save\",\n",
        "        data=output_model,\n",
        "        file_id=save_model_name,\n",
        "        project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(save)  # add to graph\n",
        "\n",
        "    # 2.g. Export torchscript node\n",
        "    export = Node(\n",
        "        node_name=\"ExportTorchScript\",\n",
        "        label=\"ExportTorchScript\",\n",
        "        model=output_model,\n",
        "        validation_inputs=input_dataset,\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(export)  # add to graph\n",
        "    export_model = export.make_handle(\"torch_script\")  # add handle\n",
        "\n",
        "    # 2.h. Download node\n",
        "    download = Node(\n",
        "        node_name=\"Download\",\n",
        "        label=\"Download\",\n",
        "        file=export_model,\n",
        "        name=save_model_name,\n",
        "        client=client,\n",
        "    )\n",
        "    graph.add_node(download)  # add to graph\n",
        "    if is_visualise_workflow:\n",
        "        pprint(graph.nodes)\n",
        "\n",
        "    workflow = Workflow(\n",
        "        graph=graph.nodes,\n",
        "        inputs=graph.external_input,\n",
        "        external_input_id=graph.external_input_id,\n",
        "        requested_output={\n",
        "            \"Torchscript\": download.make_handle(\"file\").model_dump(),\n",
        "          # \"Torchscript\": download,\n",
        "        },\n",
        "        client=client\n",
        "        )\n",
        "    try:\n",
        "        client.workflows.save(\n",
        "          project_id=client.projects.get_project_id_by_name(PROJECT_NAME),\n",
        "          workflow=workflow,\n",
        "          workflow_name=\"TrainModelExportTorchscriptModel\",\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    response = client.run_node(workflow)\n",
        "    if is_print_full_output:\n",
        "        pprint(response.model_dump())\n",
        "\n",
        "    try:\n",
        "        urllib.request.urlretrieve(response.outputs[\"outputs\"][\"Torchscript\"], f\"{save_model_name}.pt\")\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    return response.outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3nwhNBxDQih"
      },
      "outputs": [],
      "source": [
        "n_latent = 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n_zLZrXTvq1",
        "outputId": "99a18b73-97b4-47fe-a040-563e5700bd5e"
      },
      "outputs": [],
      "source": [
        "output_column_names = [f\"T_latent_{i}\" for i in range(n_latent)]\n",
        "save_model_names = output_column_names\n",
        "\n",
        "for cn, mn in zip(output_column_names, save_model_names):\n",
        "    print(f\"Modelling output: {cn}\")\n",
        "    response = train_export_torchscript_workflow(\n",
        "                        client=client,\n",
        "                      project_name=PROJECT_NAME,\n",
        "                      input_dataset_name=\"inputs\",\n",
        "                      output_dataset_name=\"outputs\",\n",
        "                        output_column_name=cn,\n",
        "                      save_model_name=mn,\n",
        "                        is_visualise_workflow=False,\n",
        "                        is_print_full_output=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PW64jr39UZs",
        "outputId": "d97ad2cd-5c72-42ea-ad41-aee8e9b54a48"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Phqgvl9RItv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Juo7jz2iBPup"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIvyapXGBU2-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp1_4K9jBRF7"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_path: str) -> torch.jit.ScriptModule:\n",
        "        \"\"\"\n",
        "        Load a TorchScript model from disk.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model_path : str\n",
        "            Path to the saved TorchScript model (.pt/.pth/.torch).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.jit.ScriptModule\n",
        "            The loaded and ready-to-use model.\n",
        "        \"\"\"\n",
        "        model = torch.jit.load(model_path, map_location=\"cpu\")\n",
        "        model.eval()\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def __call__(\n",
        "            self,\n",
        "        prediction_points: np.ndarray,\n",
        "        return_dict: bool = False\n",
        "    ) -> Union[\n",
        "        Tuple[np.ndarray, np.ndarray],\n",
        "        Dict[str, np.ndarray]\n",
        "    ]:\n",
        "        \"\"\"\n",
        "        Run prediction on a TorchScript model using NumPy input.\n",
        "\n",
        "        Converts the input NumPy array into a float32 tensor, performs inference,\n",
        "        and converts the model outputs (prediction, uncertainty) back to NumPy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : torch.jit.ScriptModule\n",
        "            A loaded TorchScript model.\n",
        "            The model is assumed to return a tuple of tensors: (prediction, uncertainty)\n",
        "\n",
        "        prediction_points : np.ndarray\n",
        "            Input data with shape (N, D).\n",
        "            Will be converted to a torch.float32 tensor.\n",
        "\n",
        "        return_dict : bool, optional\n",
        "            If True, returns {\"prediction\": ..., \"uncertainty\": ...}.\n",
        "            If False (default), returns (prediction, uncertainty) as a tuple.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict or tuple\n",
        "            If return_dict=True:\n",
        "                {\n",
        "                    \"prediction\": np.ndarray,\n",
        "                    \"uncertainty\": np.ndarray\n",
        "                }\n",
        "            Else:\n",
        "                (prediction: np.ndarray, uncertainty: np.ndarray)\n",
        "        \"\"\"\n",
        "        # Convert input NumPy array into a tensor\n",
        "        x = torch.tensor(prediction_points, dtype=torch.float32)\n",
        "        model = self.model\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)  # expected: (prediction_tensor, uncertainty_tensor)\n",
        "\n",
        "        # Convert model outputs to NumPy\n",
        "        prediction_np, uncertainty_np = (\n",
        "            t.detach().cpu().numpy() for t in y_pred\n",
        "        )\n",
        "\n",
        "        if return_dict:\n",
        "            return {\n",
        "                \"prediction\": prediction_np,\n",
        "                \"uncertainty\": uncertainty_np\n",
        "            }\n",
        "\n",
        "        return prediction_np, uncertainty_np\n",
        "\n",
        "\n",
        "class CombinedModel:\n",
        "    def __init__(self, model_filepaths):\n",
        "        self.models = [Model(model_path=fp) for fp in model_filepaths]\n",
        "\n",
        "    def __call__(self, prediction_points, return_dict=False):\n",
        "        predictions = []\n",
        "        uncertainties = []\n",
        "        for model in self.models:\n",
        "            pred, uncert = model(prediction_points, return_dict=False)\n",
        "            predictions.append(pred)\n",
        "            uncertainties.append(uncert)\n",
        "\n",
        "        # Combine predictions and uncertainties (e.g., by averaging)\n",
        "        combined_prediction = np.hstack(predictions)\n",
        "        combined_uncertainty = np.hstack(uncertainties)\n",
        "\n",
        "        if return_dict:\n",
        "            return {\n",
        "                \"prediction\": combined_prediction,\n",
        "                \"uncertainty\": combined_uncertainty\n",
        "            }\n",
        "\n",
        "        return combined_prediction, combined_uncertainty\n",
        "\n",
        "\n",
        "class FMU:\n",
        "    \"\"\"\n",
        "    Functional Mock-up Unit for block temperature evolution using a GP in latent space.\n",
        "\n",
        "    Model inputs (to GP):\n",
        "        [ convective_coef, heat_flux, ambient_temp, latent_state_at_t_minus_lag ]\n",
        "\n",
        "    Model outputs (from GP):\n",
        "        latent_state_at_current_time\n",
        "\n",
        "    The FMU:\n",
        "      - Stores the full temperature field (e.g. 4960 vertices).\n",
        "      - Maps full -> latent via forward_matrix.\n",
        "      - Maps latent -> full via backward_matrix.\n",
        "      - Maintains a history of latent states over time so it can\n",
        "        interpolate the latent state at (current_time + dt - lag_s).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : callable\n",
        "        Surrogate GP model with signature:\n",
        "            latent_mean, latent_uncertainty = model(prediction_points=features)\n",
        "        where:\n",
        "            latent_mean: shape (n_latent,) or (1, n_latent)\n",
        "            latent_uncertainty: same shape (std devs, MC std, etc.)\n",
        "    forward_matrix : np.ndarray\n",
        "        Matrix mapping full temperature state to latent:\n",
        "            z = T @ forward_matrix\n",
        "        Shape: (n_vertices, n_latent)\n",
        "    backward_matrix : np.ndarray\n",
        "        Matrix mapping latent state back to full space:\n",
        "            T_hat = z @ backward_matrix\n",
        "        Shape: (n_latent, n_vertices)\n",
        "    initial_input_state : np.ndarray\n",
        "        Initial external inputs, shape (3,) for:\n",
        "            [Convection_FilmCoef, Heat Flux_Magnitude, Convection_AmbientTemp]\n",
        "    initial_temp_state : np.ndarray\n",
        "        Initial full temperature field, shape (n_vertices,).\n",
        "    lag_s : float, optional\n",
        "        Time lag in seconds for the latent memory (default 0.1 s).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        pca_matrix: np.ndarray,\n",
        "        initial_input_state: np.ndarray,\n",
        "        initial_temp_state: np.ndarray,\n",
        "        lag_s: float = 1.,\n",
        "        n_mc: int = 32,\n",
        "        random_state = 42,\n",
        "    ):\n",
        "        self.model = model\n",
        "\n",
        "        self.backward_matrix = np.asarray(pca_matrix, dtype=float)\n",
        "        self.forward_matrix = self.backward_matrix.T\n",
        "\n",
        "        # State: full temperature field\n",
        "        self.current_temp_state = np.asarray(initial_temp_state, dtype=float).copy()\n",
        "        self.current_temp_uncertainty = np.zeros_like(self.current_temp_state)\n",
        "\n",
        "        # External inputs (3 scalars)\n",
        "        self.current_input_state = np.asarray(initial_input_state, dtype=float).copy()\n",
        "\n",
        "        # RNG for Monte Carlo\n",
        "        self.n_mc = int(n_mc)\n",
        "        self.rng = np.random.default_rng(random_state)\n",
        "\n",
        "        # Time management\n",
        "        self.current_time = 0.0\n",
        "        self.lag_s = float(lag_s)\n",
        "\n",
        "        # Initial latent state (from full field)\n",
        "        self.current_latent_state = self.current_temp_state @ self.forward_matrix  # (n_latent,)\n",
        "        self.current_latent_uncertainty = np.zeros_like(self.current_latent_state)\n",
        "\n",
        "        # History of latent states (for interpolation)\n",
        "        # times[i] corresponds to latent_history[i]\n",
        "        self.times = [self.current_time]\n",
        "        self.latent_history = [self.current_latent_state.copy()]\n",
        "        self.latent_std_history = [self.current_latent_uncertainty.copy()]\n",
        "\n",
        "    # ---------- Internal helpers ----------\n",
        "\n",
        "\n",
        "    def _interp_vector_history(self, t_query: float, times, values) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Generic linear interpolation of a vector-valued history.\n",
        "        times: list[float]\n",
        "        values: list[np.ndarray] with same shape.\n",
        "        \"\"\"\n",
        "        # Clamp if query is outside known range\n",
        "        if t_query <= times[0]:\n",
        "            return values[0].copy()\n",
        "        if t_query >= times[-1]:\n",
        "            return values[-1].copy()\n",
        "\n",
        "        idx = bisect_right(times, t_query) - 1\n",
        "        t0, t1 = times[idx], times[idx + 1]\n",
        "        v0, v1 = values[idx], values[idx + 1]\n",
        "\n",
        "        if t1 == t0:\n",
        "            return v0.copy()\n",
        "\n",
        "        w = (t_query - t0) / (t1 - t0)\n",
        "        return (1.0 - w) * v0 + w * v1\n",
        "\n",
        "    def _latent_mean_at(self, t_query: float) -> np.ndarray:\n",
        "        \"\"\"Latent mean at time t_query (linear interpolation).\"\"\"\n",
        "        return self._interp_vector_history(t_query, self.times, self.latent_history)\n",
        "\n",
        "    def _latent_std_at(self, t_query: float) -> np.ndarray:\n",
        "        \"\"\"Latent std at time t_query (linear interpolation).\"\"\"\n",
        "        return self._interp_vector_history(t_query, self.times, self.latent_std_history)\n",
        "\n",
        "    # ---------- Main step ----------\n",
        "\n",
        "    def __call__(self, new_input_state: np.ndarray, time_step: float):\n",
        "        \"\"\"\n",
        "        Advance the FMU by one time step, propagating uncertainty via MC.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        new_input_state : np.ndarray\n",
        "            External inputs at the *new* time (shape (3,)):\n",
        "                [Convection_FilmCoef, Heat Flux_Magnitude, Convection_AmbientTemp]\n",
        "        time_step : float\n",
        "            Time step size Δt (seconds). Can vary between calls.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        current_temp_state : np.ndarray\n",
        "            Updated full temperature state at the new time (n_vertices,).\n",
        "        current_temp_uncertainty : np.ndarray\n",
        "            Approximate uncertainty in full temperature state (n_vertices,).\n",
        "        \"\"\"\n",
        "        dt = float(time_step)\n",
        "        if dt <= 0.0:\n",
        "            raise ValueError(f\"time_step must be positive, got {dt}\")\n",
        "\n",
        "        new_input_state = np.asarray(new_input_state, dtype=float).copy()\n",
        "\n",
        "        # Time at the end of this step\n",
        "        new_time = self.current_time + dt\n",
        "\n",
        "        # Time where we need the latent memory\n",
        "        t_lag = new_time - self.lag_s\n",
        "\n",
        "        # Latent mean & std at (new_time - lag_s), via interpolation\n",
        "        latent_lag_mean = self._latent_mean_at(t_lag)\n",
        "        latent_lag_std = self._latent_std_at(t_lag)\n",
        "\n",
        "        n_latent = latent_lag_mean.shape[0]\n",
        "        model_inputs = np.hstack([new_input_state, latent_lag_mean])\n",
        "        latent_mean_new, _ = self.model(prediction_points=model_inputs)\n",
        "        latent_mean_new = latent_mean_new.squeeze()\n",
        "\n",
        "        # --- Monte Carlo over latent input uncertainty ---\n",
        "        latent_samples = []\n",
        "        latent_var_cond = []  # GP conditional variances per sample\n",
        "\n",
        "        for _ in range(self.n_mc):\n",
        "            # Sample latent state at t_lag\n",
        "            eps = self.rng.standard_normal(size=n_latent)\n",
        "            z_lag_sample = latent_lag_mean + latent_lag_std * eps\n",
        "\n",
        "            # Build model input: [external inputs at new_time, latent sample at t_lag]\n",
        "            model_inputs = np.hstack([new_input_state, z_lag_sample])\n",
        "\n",
        "            # Surrogate model prediction in latent space for this sample\n",
        "            # Assume model returns (mean, std) for latent_next\n",
        "            latent_mean_s, latent_std_s = self.model(prediction_points=model_inputs)\n",
        "\n",
        "            latent_mean_s = np.asarray(latent_mean_s, dtype=float).reshape(-1)\n",
        "            latent_std_s = np.asarray(latent_std_s, dtype=float).reshape(-1)\n",
        "\n",
        "            latent_samples.append(latent_mean_s)\n",
        "            latent_var_cond.append(latent_std_s**2)\n",
        "\n",
        "        latent_samples = np.stack(latent_samples, axis=0)      # (n_mc, n_latent)\n",
        "        latent_var_cond = np.stack(latent_var_cond, axis=0)    # (n_mc, n_latent)\n",
        "\n",
        "        # --- Combine samples: law of total variance ---\n",
        "        # E[z] = mean over MC of conditional means\n",
        "        # latent_mean_new = latent_samples.mean(axis=0)\n",
        "\n",
        "        # Var[z] = E[Var(z|x)] + Var(E[z|x])\n",
        "        # term1: average conditional variance\n",
        "        term1 = latent_var_cond.mean(axis=0)\n",
        "        # term2: variance of conditional means\n",
        "        term2 = latent_samples.var(axis=0, ddof=1) if self.n_mc > 1 else np.zeros_like(term1)\n",
        "\n",
        "        latent_var_new = term1 + term2\n",
        "        latent_std_new = np.sqrt(np.maximum(latent_var_new, 0.0))\n",
        "\n",
        "        # Update latent state & uncertainty\n",
        "        self.current_latent_state = latent_mean_new\n",
        "        self.current_latent_uncertainty = latent_std_new\n",
        "\n",
        "        # Map latent -> full temperature field\n",
        "        self.current_temp_state = latent_mean_new @ self.backward_matrix  # (n_vertices,)\n",
        "\n",
        "        # Approx uncertainty propagation: latent stds through |backward_matrix|\n",
        "        # (assuming independence in latent dims)\n",
        "        self.current_temp_uncertainty = latent_std_new @ np.abs(self.backward_matrix)\n",
        "\n",
        "        # Update time and inputs\n",
        "        self.current_time = new_time\n",
        "        self.current_input_state = new_input_state\n",
        "\n",
        "        # Append to history for future interpolation\n",
        "        self.times.append(new_time)\n",
        "        self.latent_history.append(self.current_latent_state.copy())\n",
        "        self.latent_std_history.append(self.current_latent_uncertainty.copy())\n",
        "\n",
        "        return self.current_temp_state, self.current_temp_uncertainty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUCfjFssC5Nq"
      },
      "outputs": [],
      "source": [
        "models = [f\"{mn}.pt\" for mn in save_model_names]\n",
        "model = CombinedModel(model_filepaths=models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTJ17OAKEDy6"
      },
      "outputs": [],
      "source": [
        "val_number = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvYZGwdQEInJ"
      },
      "outputs": [],
      "source": [
        "val_in = pd.read_csv(f\"Scenario_{val_number}_exc.csv\")\n",
        "val_out = pd.read_csv(f\"Scenario_{val_number}_out.csv\")\n",
        "val_in.columns = val_in.columns.str.strip()\n",
        "val_out.columns = val_out.columns.str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu3gMs4hENTG"
      },
      "outputs": [],
      "source": [
        "times = val_in['Time'].to_numpy()\n",
        "val_inputs = val_in[['Convection_FilmCoef', 'Heat Flux_Magnitude',\t'Convection_AmbientTemp']].to_numpy()\n",
        "cols = [str(x) for x in range(1, 4961)]\n",
        "val_outputs = val_out[cols].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6EGBPcSEaBY"
      },
      "outputs": [],
      "source": [
        "time_steps = times[1:] - times[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FOosARqEQPH"
      },
      "outputs": [],
      "source": [
        "fmu = FMU(\n",
        "    model=model,\n",
        "    pca_matrix=pca_matrix,\n",
        "    initial_input_state=val_inputs[0],\n",
        "    initial_temp_state=val_outputs[0],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG7AnCWzEWRA"
      },
      "outputs": [],
      "source": [
        "output_states = []\n",
        "output_state_uncertainties = []\n",
        "for new, t_step in zip(val_inputs[1:], time_steps):\n",
        "    new_state, new_uncertainty = fmu(new, time_step=t_step)\n",
        "    # print(new_state)\n",
        "    # print(new_uncertainty[0])\n",
        "    output_states.append(np.array(new_state, copy=True))\n",
        "    output_state_uncertainties.append(np.array(new_uncertainty, copy=True))\n",
        "\n",
        "output_states = np.array(output_states)\n",
        "output_state_uncertainties = np.array(output_state_uncertainties)\n",
        "output_lower_bound = output_states - 1.96 * output_state_uncertainties\n",
        "output_upper_bound = output_states + 1.96 * output_state_uncertainties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkN2MN5OEdmx"
      },
      "outputs": [],
      "source": [
        "# reshape\n",
        "times_plot = times[1:]\n",
        "val_outputs_plot = val_outputs[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lGB5wb0Egc1"
      },
      "outputs": [],
      "source": [
        "plot_vertex = 300\n",
        "\n",
        "plt.plot(times_plot, val_outputs_plot[:, plot_vertex], label=f\"Vertex {plot_vertex} Temp\", linestyle='dashed')\n",
        "plt.plot(times_plot, output_states[:, plot_vertex], label=f\"Vertex {plot_vertex} Temp\")\n",
        "plt.fill_between(times_plot,\n",
        "                 output_lower_bound[:, plot_vertex],\n",
        "                 output_upper_bound[:, plot_vertex],\n",
        "                 color='blue', alpha=0.2, label=\"Uncertainty\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"Temperature (K)\")\n",
        "plt.suptitle(f\"Block UQ Model Prediction vs True Temperature.\\nVertex {plot_vertex}; Val Scenario {val_number}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
